# Neural network configurations for different prop types

receptions:
  hidden_layers: [128, 64, 32]
  dropout_rate: 0.3
  learning_rate: 0.001
  batch_size: 32
  epochs: 100
  early_stopping_patience: 10
  activation: "relu"
  optimizer: "adam"
  loss: "mse"

passing_yards:
  hidden_layers: [256, 128, 64]
  dropout_rate: 0.4
  learning_rate: 0.0005
  batch_size: 64
  epochs: 150
  early_stopping_patience: 15
  activation: "relu"
  optimizer: "adam"
  loss: "mse"

rushing_yards:
  hidden_layers: [128, 64, 32]
  dropout_rate: 0.3
  learning_rate: 0.001
  batch_size: 32
  epochs: 100
  early_stopping_patience: 10
  activation: "relu"
  optimizer: "adam"
  loss: "mse"

receiving_yards:
  hidden_layers: [128, 64, 32]
  dropout_rate: 0.3
  learning_rate: 0.001
  batch_size: 32
  epochs: 100
  early_stopping_patience: 10
  activation: "relu"
  optimizer: "adam"
  loss: "mse"

anytime_td:
  hidden_layers: [64, 32, 16]
  dropout_rate: 0.4
  learning_rate: 0.001
  batch_size: 64
  epochs: 80
  early_stopping_patience: 10
  activation: "relu"
  optimizer: "adam"
  loss: "binary_crossentropy"

# LSTM configuration for time series features
lstm:
  sequence_length: 5  # Number of previous games to consider
  lstm_units: 50
  dense_layers: [32, 16]
  dropout_rate: 0.3
  recurrent_dropout: 0.2
  learning_rate: 0.001
  batch_size: 32
  epochs: 100

# Hyperparameter optimization ranges
hyperopt_ranges:
  hidden_layer_size: [32, 256]
  num_hidden_layers: [2, 4]
  dropout_rate: [0.1, 0.5]
  learning_rate: [0.0001, 0.01]
  batch_size: [16, 128]